{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa56fcbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m encoder_embedding \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(encoder_inputs)\n\u001b[0;32m     46\u001b[0m encoder_lstm \u001b[38;5;241m=\u001b[39m LSTM(\u001b[38;5;241m256\u001b[39m, return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m encoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m encoder_states \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Decoder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VAISHALI\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\VAISHALI\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 256)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. Prepare the dataset (English ↔ Hindi)\n",
    "english_sentences = [\n",
    "    \"I am happy\", \"You are beautiful\", \"She is reading\", \"We are learning\", \"They are working\",\n",
    "    \"I love you\", \"He is playing\", \"She loves me\", \"I am studying\", \"We are talking\"\n",
    "]\n",
    "\n",
    "hindi_sentences = [\n",
    "    \"मैं खुश हूँ\", \"तुम सुंदर हो\", \"वह पढ़ रही है\", \"हम सीख रहे हैं\", \"वे काम कर रहे हैं\",\n",
    "    \"मैं तुमसे प्यार करता हूँ\", \"वह खेल रहा है\", \"वह मुझसे प्यार करती है\", \"मैं पढ़ाई कर रहा हूँ\", \"हम बात कर रहे हैं\"\n",
    "]\n",
    "\n",
    "# 2. Preprocess the data\n",
    "# Tokenize English\n",
    "english_tokenizer = Tokenizer()\n",
    "english_tokenizer.fit_on_texts(english_sentences)\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1  # +1 for padding\n",
    "\n",
    "# Tokenize Hindi\n",
    "hindi_tokenizer = Tokenizer()\n",
    "hindi_tokenizer.fit_on_texts(hindi_sentences)\n",
    "hindi_sequences = hindi_tokenizer.texts_to_sequences(hindi_sentences)\n",
    "hindi_vocab_size = len(hindi_tokenizer.word_index) + 1  # +1 for padding\n",
    "\n",
    "# Pad the sequences\n",
    "max_input_length = max([len(seq) for seq in english_sequences])\n",
    "max_output_length = max([len(seq) for seq in hindi_sequences])\n",
    "\n",
    "english_sequences = pad_sequences(english_sequences, maxlen=max_input_length, padding='post')\n",
    "hindi_sequences = pad_sequences(hindi_sequences, maxlen=max_output_length, padding='post')\n",
    "\n",
    "# Prepare decoder input and output\n",
    "hindi_input = hindi_sequences[:, :-1]  # remove last token\n",
    "hindi_output = hindi_sequences[:, 1:]  # remove first token\n",
    "\n",
    "# 3. Define the Encoder-Decoder model\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_input_length,))\n",
    "encoder_embedding = Dense(256, activation='relu')(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_output_length - 1,))\n",
    "decoder_embedding = Dense(256, activation='relu')(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(hindi_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Train the model\n",
    "model.fit([english_sequences, hindi_input], np.expand_dims(hindi_output, -1), epochs=100, batch_size=16)\n",
    "\n",
    "# 5. Translate an English sentence to Hindi\n",
    "def translate_sentence(input_sentence):\n",
    "    input_sequence = english_tokenizer.texts_to_sequences([input_sentence])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_input_length, padding='post')\n",
    "\n",
    "    # Encode the input sentence\n",
    "    encoder_output, state_h, state_c = encoder_lstm(input_sequence)\n",
    "\n",
    "    # Prepare the first token for decoding\n",
    "    target_sequence = np.zeros((1, 1))\n",
    "    target_sequence[0, 0] = hindi_tokenizer.word_index['starttoken']  # Special token for start\n",
    "\n",
    "    translated_sentence = \"\"\n",
    "    \n",
    "    while True:\n",
    "        # Predict the next token\n",
    "        decoder_output, _, _ = decoder_lstm(target_sequence, initial_state=[state_h, state_c])\n",
    "        decoder_probs = decoder_dense(decoder_output)\n",
    "        \n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(decoder_probs[0, -1, :])\n",
    "        sampled_token = hindi_tokenizer.index_word[sampled_token_index]\n",
    "\n",
    "        # Stop if end token or sentence is complete\n",
    "        if sampled_token == 'endtoken' or len(translated_sentence.split()) >= max_output_length:\n",
    "            break\n",
    "\n",
    "        # Append the token to the translated sentence\n",
    "        translated_sentence += \" \" + sampled_token\n",
    "\n",
    "        # Update target sequence\n",
    "        target_sequence = np.zeros((1, 1))\n",
    "        target_sequence[0, 0] = sampled_token_index\n",
    "\n",
    "    return translated_sentence\n",
    "\n",
    "# Translate a sample sentence\n",
    "sample_sentence = \"I am happy\"\n",
    "print(f\"English: {sample_sentence}\")\n",
    "print(f\"Hindi: {translate_sentence(sample_sentence)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
